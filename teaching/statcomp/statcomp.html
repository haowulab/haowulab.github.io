<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"> <html>
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">

  <style type="text/css">
  ADDRESS {font-family: Arial, Helvetica;}
  BODY {margin-top:5%; margin-left:10%;margin-right:10%;margin-bottom:10%;
  font-family: Arial, Helvetica; line-height:130%}
  H1{line-height:120%}
  TD {font-family: Arial, Helvetica;}
  P {font-family: Arial, Helvetica; text-align: justify;}
  A:link {text-decoration:none;}
  A:vlink {text-decoration:none;}
  </style>
  
  
  <title>Advanced Statistical Computing</title>
  </head>
<body width="80%">
<a name="Outline"></a>
<h1><center>BIOS731: Advanced Statistical Computing</h1>

<br><hr size="5" width="100%">
<!--
<table width="100%">
  <tbody>
    <tr>
      <th width="20%"><a href="#info">Class Info</a></th>
      <th width="20%"><a href="#announcements">Reading materials</a></th>
      <th width="20%"><a href="#class">Class schedule and notes</a></th>
    </tr>
  </tbody>
</table>

<br><hr size="5" width="100%"><br>
<a name="info">
-->

<h3>Class Information</h3>
</a>
<ul>
  <li><b>Course title</b>: Advanced Statistical Computing</li>
  <li><b>Instructors</b>: 
    <a href="http://www.haowulab.org">Hao Wu</a> (hao.wu at emory dot edu),
    <a href="http://steveqinlab.org">Zhaohui Steve Qin</a> (zhaohui.qin at emory dot edu).
  <li><b>TA</b>: Luxiao Chen (luxiao.chen AT emory DOT edu)
  </li><li><b>Class</b>: Tuesday & Thursday 3:00-4:50PM at <b>GCR 119</b>. 
  </li><li><b>Office hour and location</b>: Make appointment, or email question

</ul>

<center><hr size="5" width="100%"></center>
<h3>Summary</h3>
This course is designed for Ph.D. level biostatistics students.
It covers the methods and applications of some common statistical computing methods. 
Topics include random number generation, permutation and bootstrap,
optimization methods, Expectation-Maximization (EM), Minorization-Maximization (MM),
linear/quadratic programming, hidden Markov model (HMM), and
Markov chain Monte Carlo (MCMC). 
<p>
The class has two main goals for students: 
(1) learn the general theory and algorithmic procedures of several widely 
used statistical models; (2) develop fluency in statistical programming skills. 
The class puts more emphasis on implementation instead of statistical theories,
although a non-trivial amount of theories will be taught in the class.
Students will gain computational skills and practical experiences on simulations and statistical modeling. 
<p>
This course requires significant amount of programming. Each set of homework involves 
the analysis of real dataset or implementation of certain algorithms 
using high-level programming language (Matlab/R/perl/python). 
<p>
Upon successfully completing this course, students will be able to:
<ol>
<li>Understand the general theories and computational procedures of the algorithms covered in the class.
<li>Implement the algorithms using high-level programming languages.
<li>Apply the methods to model real world data. 
</ol>
<p>
The <b>target audiences</b> of the class include Biostatistics and Bioinformatics Ph.D. students, 
or other graduate students who works on highly quantitative fields. 
<p>
<b>Prerequisites</b>: BIOS 510 or equivalent. BIOS 545 “Introduction to R programming” or 
prior programming experiences in one or more high-level programming languages (Matlab/R/perl/python).
<p>
<b>Grading</b>: Four sets of homework for a total of 90%. Class attendence and participation, 10%.


<center><hr size="5" width="100%"></center>

<!-- announcements -->
<h3>Recommended books:</h3> </a><p>
<ol>
<li>Lange, K., <a href="http://www.amazon.com/Numerical-Analysis-Statisticians-Statistics-Computing/dp/1441959440/ref=sr_1_1?ie=UTF8&qid=1345654374&sr=8-1&keywords=Numerical+Analysis+for+Statisticians"><i>Numerical Analysis for Statisticians</i></a>.
<li>Gilks W. R. et al., <a href="http://www.amazon.com/Markov-Practice-Chapman-Interdisciplinary-Statistics/dp/0412055511"><i>Markov chain Monte Carlo in Practice</i></a>.
<li>Press WH, et al., <a href="http://www.nr.com/"><i>Numerical Recipes in C</i></a>. 
<li>Durbin, R. et al., <a href="http://www.amazon.com/Biological-Sequence-Analysis-Probabilistic-Proteins/dp/0521629713/ref=sr_1_1?s=books&ie=UTF8&qid=1346348980&sr=1-1"><i>Biological sequence analysis</i></a>.
</ol>


<center><hr size="5" width="100%"></center>
<p><a name="class"><h3>Class schedule and notes</h3> </a>
<table cellpadding=3 cellspacing=3 border=1>
<tr>
<td>Date</td>
<td>Lecture Title</td>
<td>Description</td>
<td>Homework</td>
</tr>

<tr>
<td>8/25 (Thurs)</td>
<td><b>Lecture 1: Random number generation, permutation and bootstrap</b> (Wu) 
[<a href="Notes/lecture1.pdf">Notes</a>]</td>
<td>Course information. 
Random number generation. permutation and bootstrap.
</td>
<td></td>
</tr>

<td>8/30 (Tues)</td>
<td><b>Lecture 2: Optimization methods</b> (Wu)
[<a href="Notes/optimization.pdf">Notes</a>]
<td>Optimization Algorithms: Newton-Raphson, Quasi-Newton-Raphson, 
Iteratively Re-weighted Least Squares for Generalized Linear Regression</td>
<td><a href="homework/homework1.pdf">homework1</a></td>
</tr>

<td>9/1 (Thurs)</td>
<td><b>Lecture 3: EM algorithm I</b> (Wu)
[<a href="Notes/EM1.pdf">Notes</a>]
<td>EM algorithm and applications.</td>
<td></td>
</tr>

<td>9/6 (Tues)</td>
<td><b>Lecture 4: EM algorithm II</b> (Wu) 
[<a href="Notes/EM2.pdf">Notes</a>]
</td>
<td>EM algorithm extensions, SEM algorithm, EM gradient algorithm, ECM algorithm.</td>
<td></td>
</tr>

<td>9/8 (Thurs)</td>
<td><b>Lecture 5: MM algorithm</b> (Wu) 
[<a href="Notes/MM.pdf">Notes</a>]</td>
<td>MM algorithm and applications</td>
<td><a href="homework/homework1.pdf">homework2</a></td>
</tr>

<td>9/13 (Tues)</td>
<td><b>Lecture 6: HMM I</b> (Wu)
[<a href="Notes/HMM1.pdf">Notes</a>]</td>
<td>Introduction to HMM. Forward-backward algorithm.</td>
<td></td>
</tr>

<td>9/15 (Thurs)</td>
<td><b>Lecture 7: HMM II</b> (Wu) 
[<a href="Notes/HMM2.pdf">Notes</a>]</td>
<td>Viterbi algorithm.</a>
<td></td>
<tr>


<td>9/20 (Tues)</td>
<td><b>Lecture 8: Linear programming I</b> (Wu) 
[<a href="Notes/lp1.pdf">Notes</a>]</td>
<td>Introduction. Simplex algorithm. Duality. </td>
<td></td>
</tr>

<td>9/22 (Thurs)</td>
<td><b>Lecture 9: Linear programming II</b> (Wu) 
[<a href="Notes/lp2.pdf">Notes</a>]</td>
<td>Interior point method. Quadratic programming. Applications of LP: quantile regression.</td>
<td></td>
</tr>

<td>9/27 (Tues)</td>
<td><b>Lecture 10: Linear programming III</b> (Wu) 
[<a href="Notes/lp3.pdf">Notes</a>]</td>
<td>Applications of LP: LASSO, Support vector machine.</td>
<td><a href="homework/homework2.pdf">homework3</a>, 
<a href="price.txt">Data for questions 1</a></td>
</tr>

<td>9/29 (Thur)</td>
<td><b>Lecture 11: MCMC I </b> (Qin) 
[<a href="Notes/MCMC1.pdf">Notes</a>]</td>
<td>Introduction to MCMC.</td>
<td></td>
</tr>

<!-- <td>10/9 (Tues)</td>
<td>Fall break, no class</td>
<td></td>
<td></td>
</tr>
-->

<td>10/4 (Tues)</td>
<td><b>Lecture 12: MCMC II</b> (Qin) 
[<a href="Notes/MCMC2.pdf">Notes</a>]</td>
<td>Introduction to sequential MC.</td>
<td></td>
</tr>

<td>10/6 (Thur)</td>
<td><b>Lecture 13:  MCMC III </b>(Qin)
[<a href="Notes/MCMC3.pdf">Notes</a>, 
<a href="Notes/mcmc_handout.doc">handout</a>]</td>
<td>Other techniques, applications of MCMC.</td>
<td><a href="homework/homework3.pdf">homework4</a></td>
</tr>


<td>10/11 (Tues)</td>
<td><b>Fall break, no class</b></td>
</tr>


<td>10/13 (Thur)</td>
<td><b>Lecture 14:  Dimension reduction </b>(Wu)
[<a href="Notes/DimensionReduction.pdf">Notes</a>]</td>
<td>Introduce two dimension reduction methods: non-negative matrix factorization (NMF),
and local Dirichlet allocation (LDA). </td>
<td></td>
</tr>


<td>10/18 (Thur)</td>
<td><b>Lecture 14:  Introduction to deep learning </b>
[<a href="Notes/DeepLearning.pdf">Notes</a>]</td>
<td>Perceptron, neural network, deep architect, convolutional neural network, transfer learning.</td>
<center><hr size="5" width="100%"></center>


</table>

<h3>Additinal readings and resource:</h3><p>
<ul><b>Papers</b>:
<ol>
<li>Bootstrap:
<ul>
<li><a href="papers/Efron1979_bootstrap.pdf">Efron boostrap AOS 1979</a>
<li><a href="papers/BLB.pdf">Bag of little bootstraps JRSSB 2014</a>
</ul>

<li>EM and MM:
<ul>
<li><a href="papers/DLR_EM.pdf">The DLR paper</a>, 
<li><a href="papers/EM-mixtureModel.pdf">A tutorial paper on EM algorithm</a>,
<li><a href="papers/EM_converge.pdf">EM convergence</a>.
<li><a href="papers/mmtutorial.pdf">A tutorial on MM algorithm</a>
<li><a href="papers/MM_applications.pdf">MM applications</a>.
</ul>
<li>HMM: 
<ul>
<li><a href="papers/hmm-tutorial-rabiner.pdf">A tutorial on HMM</a>. 
</ul>

<li>MCMC: 
<ul>
<li><a href="papers/ramaley.Buffon.69.pdf">Buffon's Needle Problem</a>
<li><a href="papers/MCMCHistory.pdf">Short history of MCMC</a>
<li><a href="papers/diaconis.pdf">The MCMC revolution</a>
<li><a href="papers/casellaGeorgeGibbs.pdf">Explaining the Gibbs Sampler</a>
<li><a href="papers/chibGreenbergMH.pdf">Understanding the Metropolis-Hastings Algorithm</a>
</ul>

<li>Neural network and deep learning:
<ul>
<li><a href="papers/fastnc.pdf">A fast learning algorithm for deep belief nets</a>.
<li><a href="papers/BengioNips2006All.pdf">Greedy Layer-Wise Training of Deep Networks</a>.
<li><a href="papers/DL_review_nature2015.pdf">A DL review paper on Nature</a>.

</ul>
</ol>
</ul>

<ul><b>Online meterials</b>:
<ol>
<li>R programming: <a href="http://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf">
R for beginners</a> is highly recommended. 
<li><a href="http://cran.us.r-project.org/">CRAN R archive</a>.
<li><a href="http://cran.us.r-project.org/doc/FAQ/R-FAQ.html">R FAQ</a>.
<!--<li><a href="http://big-at-emory.wikispaces.com/SPHcluster">Tips of using RSPH cluster</a>.
<li><a href="http://big-at-emory.wikispaces.com/linuxcmd">Some basic linux commands</a>. -->
<li><a href="http://luc.devroye.org/rng.html">Luc Devroye’s website on random number generation</a>.
<li>Devroye L., <a href="http://luc.devroye.org/rnbookindex.html">Non-Uniform Random Variate Generation</a>.
<li><a href="papers/SVM_R.pdf">Support Vector Machines in R</a>.
<li><a href="http://web1.sph.emory.edu/users/hwu30/computing.html">Some useful computing tips, including 
using RSPH cluster, basic linux commands, etc.</a>
</ol>
</ul>



</body></html>
